{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Reformer_Axial_Pos.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5whCRc_8ds7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "c2267bf8-b724-4065-a61b-e2006756c563"
      },
      "source": [
        "#@title Installs and Imports\n",
        "# pip installs\n",
        "!pip -qq install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -qq py3nvml\n",
        "\n",
        "from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments, ReformerModel"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLMgZt_38dtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78a62f5-53e0-45be-9593-99110c6dbc31"
      },
      "source": [
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(32, 224), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024, 2048,4096,8192,16384,32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x32]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x224]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2600448\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             0.01     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.018     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096           0.033     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192           0.062     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            0.12     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.238     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             0.01     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.018     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096           0.033     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192           0.063     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384           0.122     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768           0.241     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1931     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            2011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            2161     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096            2495     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192            3137     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            4477     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             1423     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1507     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1649     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096            1959     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192            2707     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384            4031     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIog0fxEjfFs",
        "outputId": "88220d5a-7e00-4106-b9a6-32d38598412f"
      },
      "source": [
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(86, 170), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024, 2048,4096,8192,16384,32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x86]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x170]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2572800\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.009     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.018     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096           0.033     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192           0.062     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            0.12     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.239     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.009     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.018     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096           0.033     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192           0.063     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384           0.122     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768           0.241     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1931     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            2011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            2161     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096            2495     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192            3137     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            4477     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             1423     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1507     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1649     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096            1959     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192            2707     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384            4031     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2k8sSF0jtSj",
        "outputId": "f3107938-4acb-46f4-d2fa-0b823fafc579"
      },
      "source": [
        "# 128，128\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(128, 128), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024, 2048,4096,8192,16384,32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x128]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x128]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2551296\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.009     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.018     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096           0.033     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192           0.062     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            0.12     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.239     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.018     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096           0.033     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192           0.063     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384           0.121     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768           0.241     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1931     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            2011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            2161     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096            2495     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192            3137     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            4477     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             1423     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1507     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1649     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096            1959     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192            2707     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384            4031     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOKO6h3_kAZF",
        "outputId": "a51d4acf-3df9-44bf-c7fb-2aa632ca2bc2"
      },
      "source": [
        "# 64，192\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(64, 192), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024, 2048,4096,8192,16384,32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x64]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x192]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2584064\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             0.01     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.018     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096           0.033     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192           0.062     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            0.12     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.238     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.009     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.018     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096           0.033     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192           0.063     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384           0.122     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768           0.241     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1931     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            2011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            2161     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096            2495     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192            3137     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            4477     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             1423     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1507     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1649     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096            1959     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192            2707     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384            4031     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXYGO_dekAb6",
        "outputId": "1101a905-a726-4128-d8be-63b5da6d1f24"
      },
      "source": [
        "# 192,64，\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(192, 64), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024, 2048,4096,8192,16384,32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x192]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x64]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2518528\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.009     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.018     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096           0.033     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192           0.062     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            0.12     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.239     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             0.01     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.018     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096           0.033     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192           0.062     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384           0.121     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            0.24     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1931     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            2011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            2161     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              4096            2495     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              8192            3137     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             16384            4477     \n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             1423     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1507     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1649     \n",
            "Reformer-Axial-Pos-Embeddings        8              4096            1959     \n",
            "Reformer-Axial-Pos-Embeddings        8              8192            2707     \n",
            "Reformer-Axial-Pos-Embeddings        8             16384            4031     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQRrV--UmANM",
        "outputId": "77e15a33-8e00-4458-f455-686683a896f8"
      },
      "source": [
        "# 192,64，\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(255, 1), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x255]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x1]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2486272\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768           0.239     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768           0.239     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6991     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgIYBOvnmAPY",
        "outputId": "57fdbd16-f697-48aa-f6c9-c105e46d1dcc"
      },
      "source": [
        "# reformer-crime-and-punishment\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False, max_position_embeddings=32768)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(128, 128), axial_pos_shape=(32, 1024), max_position_embeddings=32768)  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[32768], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(32768, 256)\n",
            ")\n",
            "Num parameters of model: 10743296\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 32x1x128]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x128]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2489856\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            0.24     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            0.24     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8             32768            6511     \n",
            "Reformer-Axial-Pos-Embeddings        8             32768            6737     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEG0GHBlkAda"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKNCSvDEp9kv",
        "outputId": "916c376b-8df7-4f34-9cb5-d17114dc656e"
      },
      "source": [
        "# reformer-crime-and-punishment\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(128, 128), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024,2048], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x128]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x128]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2551296\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.011     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            0.02     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           0.036     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.011     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            0.02     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           0.037     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             1501     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            1581     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            1731     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             993      \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            1077     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            1219     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJc_utmVsPxC",
        "outputId": "b5d3ef70-e404-478f-94a0-60d873f45ac0"
      },
      "source": [
        "\n",
        "# ---------------------------------------reformer-enwik8-----------------------------------\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", axial_pos_embds=True, axial_pos_embds_dim=(256, 768), axial_pos_shape=(512, 128))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024,2048], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(65536, 1024)\n",
            ")\n",
            "Num parameters of model: 215336960\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x256]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x128x768]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 148457472\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.299     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.579     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           1.137     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.299     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024           0.581     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           1.139     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             3399     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            5059     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            8385     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             3147     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            4843     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            8209     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWibBTMvPUQ7",
        "outputId": "6fac7b46-5181-4c9f-c4c4-e083c2c247b9"
      },
      "source": [
        "\n",
        "# ---------------------------------------reformer-enwik8-----------------------------------\n",
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", axial_pos_embds=True, axial_pos_embds_dim=(512, 512), axial_pos_shape=(512, 128))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(50 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(50 * '-' + '\\n\\n')\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512,1024,2048], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], speed=True, env_print=False)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(65536, 1024)\n",
            ")\n",
            "Num parameters of model: 215336960\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------------------------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x512]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x128x512]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 148555776\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512            0.299     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024           0.581     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048           1.137     \n",
            "Reformer-Axial-Pos-Embeddings        8              512            0.299     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            0.58     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048           1.138     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             3399     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              1024            5059     \n",
            "Reformer-No-Axial-Pos-Embeddin       8              2048            8385     \n",
            "Reformer-Axial-Pos-Embeddings        8              512             3147     \n",
            "Reformer-Axial-Pos-Embeddings        8              1024            4843     \n",
            "Reformer-Axial-Pos-Embeddings        8              2048            8209     \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmJGKlpSqwa0",
        "outputId": "32e8b3bc-c643-4ad9-e8a6-552ebc41019f"
      },
      "source": [
        "print(config_no_pos_axial_embeds)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ReformerConfig {\n",
            "  \"architectures\": [\n",
            "    \"ReformerModelWithLMHead\"\n",
            "  ],\n",
            "  \"attention_head_size\": 64,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attn_layers\": [\n",
            "    \"local\",\n",
            "    \"lsh\",\n",
            "    \"local\",\n",
            "    \"lsh\",\n",
            "    \"local\",\n",
            "    \"lsh\"\n",
            "  ],\n",
            "  \"axial_norm_std\": 1.0,\n",
            "  \"axial_pos_embds\": false,\n",
            "  \"axial_pos_embds_dim\": [\n",
            "    64,\n",
            "    192\n",
            "  ],\n",
            "  \"axial_pos_shape\": [\n",
            "    512,\n",
            "    1024\n",
            "  ],\n",
            "  \"chunk_size_lm_head\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feed_forward_size\": 512,\n",
            "  \"hash_seed\": null,\n",
            "  \"hidden_act\": \"relu\",\n",
            "  \"hidden_dropout_prob\": 0.05,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"local_attention_probs_dropout_prob\": 0.05,\n",
            "  \"local_attn_chunk_length\": 64,\n",
            "  \"local_num_chunks_after\": 0,\n",
            "  \"local_num_chunks_before\": 1,\n",
            "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
            "  \"lsh_attn_chunk_length\": 64,\n",
            "  \"lsh_num_chunks_after\": 0,\n",
            "  \"lsh_num_chunks_before\": 1,\n",
            "  \"max_position_embeddings\": 524288,\n",
            "  \"model_type\": \"reformer\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_buckets\": [\n",
            "    64,\n",
            "    128\n",
            "  ],\n",
            "  \"num_chunks_after\": 0,\n",
            "  \"num_chunks_before\": 1,\n",
            "  \"num_hashes\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 100\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 320\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keHpyGLFqzYA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}